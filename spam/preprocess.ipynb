{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xóa cột không cần thiết\n",
    "df.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\", \"Unnamed: 5\", \n",
    "                 \"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\", \"Unnamed: 9\", \n",
    "                 \"Unnamed: 10\", \"Unnamed: 11\", \"Unnamed: 12\", \"Unnamed: 13\", \n",
    "                 \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label = df # tập dữ liệu không có label thỏa mãn nhưng có text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label.dropna(subset=[\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = [\"spam\", \"ham\"]\n",
    "no_label = no_label[~no_label[\"label\"].isin(valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label.to_csv(\"no_label_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xóa toàn bộ dữ liệu chứa rỗng\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xóa toàn bộ dữ liệu bị lặp\n",
    "df = df.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xóa toàn bộ dữ liệu không thỏa\n",
    "valid = [\"spam\", \"ham\"]\n",
    "df = df[df[\"label\"].isin(valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thay nhãn dán thành số tương ứng\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "- chuyển về in thường\n",
    "- tokenize data\n",
    "- xóa ký tự đặc biệt\n",
    "- xóa stop-words và dấu câu\n",
    "- stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # in thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # xóa phân cách subject ở đầu text\n",
    "    if(not text.find(\"subject\")):\n",
    "        index = len(\"subject\")\n",
    "        text = text[index:]\n",
    "    \n",
    "    # xóa dấu câu\n",
    "    new_text = \"\"\n",
    "    for i in text:\n",
    "        if (i not in string.punctuation):\n",
    "            new_text += i\n",
    "    # token\n",
    "    new_text = nltk.word_tokenize(new_text)\n",
    "    res = []\n",
    "    ps = PorterStemmer()\n",
    "    for i in new_text:\n",
    "        # bỏ stop-word\n",
    "        if (i in stopwords.words(\"english\")):\n",
    "            continue\n",
    "        elif (i.isalnum()):\n",
    "            # stemming\n",
    "            res.append(ps.stem(i))\n",
    "        \n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text(\"Do you like my loved pancake on the second table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df[\"text\"].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df[\"label\"].value_counts(), labels=[\"ham\", \"spam\"], autopct=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# số ký tự trong nội dung mail\n",
    "df[\"num_characters\"] = df[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# số từ trong nội dung mail\n",
    "df[\"num_words\"] = df[\"text\"].apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# số câu trong nội dung mail\n",
    "df[\"num_sentences\"] = df[\"text\"].apply(lambda x: len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# số từ quan trọng trong nội dung mail\n",
    "df[\"num_sign_words\"] = df[\"processed_text\"].apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân bố số lượng ký tự, từ, câu\n",
    "num = [\"label\", \"num_characters\", \"num_words\", \"num_sentences\", \"num_sign_words\"]\n",
    "df_num = pd.DataFrame(df, columns=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân bố số ký tự trong nội dung\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.histplot(df_num[df[\"label\"] == 0][\"num_characters\"])\n",
    "sns.histplot(df_num[df[\"label\"] == 1][\"num_characters\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân bố số từ trong nội dung\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.histplot(df_num[df[\"label\"] == 0][\"num_words\"])\n",
    "sns.histplot(df_num[df[\"label\"] == 1][\"num_words\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân bố số câu trong nội dung\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.histplot(df_num[df[\"label\"] == 0][\"num_sentences\"])\n",
    "sns.histplot(df_num[df[\"label\"] == 1][\"num_sentences\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phân bố số từ quan trọng trong nội dung\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.histplot(df_num[df[\"label\"] == 0][\"num_sign_words\"])\n",
    "sns.histplot(df_num[df[\"label\"] == 1][\"num_sign_words\"], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_num, hue=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xu hướng của label theo số lượng ký tự, từ, câu\n",
    "sns.heatmap(df_num.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam words list\n",
    "spam_corpus = []\n",
    "for msg in df[df[\"label\"] == 1][\"processed_text\"].tolist():\n",
    "    for word in msg:\n",
    "        spam_corpus.append(word)\n",
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sns.barplot(x = pd.DataFrame(Counter(spam_corpus).most_common(30))[0], y = pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham words list\n",
    "ham_corpus = []\n",
    "for msg in df[df[\"label\"] == 0][\"processed_text\"].tolist():\n",
    "    for word in msg:\n",
    "        ham_corpus.append(word)\n",
    "len(ham_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sns.barplot(x = pd.DataFrame(Counter(ham_corpus).most_common(30))[0], y = pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building\n",
    "### 4.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(df[\"processed_text\"]).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"label\"].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train, Y_train)\n",
    "Y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(Y_test, Y_pred1))\n",
    "print(confusion_matrix(Y_test, Y_pred1))\n",
    "print(precision_score(Y_test, Y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train, Y_train)\n",
    "Y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(Y_test, Y_pred2))\n",
    "print(confusion_matrix(Y_test, Y_pred2))\n",
    "print(precision_score(Y_test, Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train, Y_train)\n",
    "Y_pred3 = mnb.predict(X_test)\n",
    "print(accuracy_score(Y_test, Y_pred3))\n",
    "print(confusion_matrix(Y_test, Y_pred3))\n",
    "print(precision_score(Y_test, Y_pred3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
